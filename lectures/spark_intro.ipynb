{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57787413",
   "metadata": {},
   "source": [
    "# Intro Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "524d5320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to Cassandra\n",
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster(['localhost'], port=9042)\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3a6e853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x2f2950b0190>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up new keyspace (first time only)\n",
    "#                                              name of keyspace                        replication strategy           replication factor\n",
    "session.execute(\"CREATE KEYSPACE IF NOT EXISTS my_first_keyspace WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e777bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables for PySpark (system and version dependent!) \n",
    "# if not already set persistently (e.g., in .bashrc or .bash_profile or Windows environment variables)\n",
    "import os\n",
    "# Set the Java home path to the one you are using ((un)comment and edit as needed):\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Java\\jre1.8.0_471\"\n",
    "\n",
    "# If you are using environments in Python, you can set the environment variables like the alternative below.\n",
    "# The default Python environment is used if the variables are set to \"python\" (edit if needed):\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python\" # or similar to \"/Users/kristian/miniforge3/envs/tf_M1/bin/python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python\" # or similar to \"/Users/kristian/miniforge3/envs/tf_M1/bin/python\"\n",
    "\n",
    "# On Windows you need to specify where the Hadoop drivers are located (uncomment and edit if needed):\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\Users\\saraa\\Documents\\winutils\\hadoop-3.3.1\"\n",
    "\n",
    "# Set the Hadoop version to the one you are using, e.g., none:\n",
    "os.environ[\"PYSPARK_HADOOP_VERSION\"] = \"without\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffb4d0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkCassandraApp').\\\n",
    "    config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.5.1').\\\n",
    "    config('spark.cassandra.connection.host', 'localhost').\\\n",
    "    config('spark.sql.extensions', 'com.datastax.spark.connector.CassandraSparkExtensions').\\\n",
    "    config('spark.sql.catalog.mycatalog', 'com.datastax.spark.connector.datasource.CassandraCatalog').\\\n",
    "    config('spark.cassandra.connection.port', '9042').\\\n",
    "    config('spark.driver.host', 'localhost').\\\n",
    "    config('spark.driver.bindAddress', '127.0.0.1').\\\n",
    "    config('spark.sql.adaptive.enabled', 'true').\\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f933549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+\n",
      "|ind| company|  model|\n",
      "+---+--------+-------+\n",
      "|  3|Polestar|      3|\n",
      "|  2|   Tesla|Model 3|\n",
      "|  1|   Tesla|Model S|\n",
      "+---+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# .load() is used to load data from Cassandra table as a Spark DataFrame\n",
    "spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"my_first_table\", keyspace=\"my_first_keyspace\").load().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d66bee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create view for simpler SQL queries\n",
    "spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"table_with_uuid\", keyspace=\"my_first_keyspace\").load().createOrReplaceTempView(\"my_first_table_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe3ed5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+\n",
      "| planet| distance| diameter|\n",
      "+-------+---------+---------+\n",
      "|Mercury| 0.387 AU|  4878 km|\n",
      "|  Venus| 0.723 AU| 12104 km|\n",
      "|  Earth| 1.000 AU| 12756 km|\n",
      "|   Mars| 1.524 AU|  6787 km|\n",
      "|Jupiter| 5.203 AU|142796 km|\n",
      "| Saturn| 9.546 AU|120660 km|\n",
      "| Uranus|19.218 AU| 51118 km|\n",
      "|Neptune|30.069 AU| 48600 km|\n",
      "+-------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read CSV file into Spark DataFrame\n",
    "planets = spark.read.csv(\"../data/planets.csv\", header=True, inferSchema=True)\n",
    "planets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0529d536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>company</th>\n",
       "      <th>model</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5595c390-a8e9-11f0-80f2-4d621f880c7b</td>\n",
       "      <td>Oldsmobile</td>\n",
       "      <td>Model 6C</td>\n",
       "      <td>135000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>558fa910-a8e9-11f0-80f2-4d621f880c7b</td>\n",
       "      <td>Tesla</td>\n",
       "      <td>Model S</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55932b80-a8e9-11f0-80f2-4d621f880c7b</td>\n",
       "      <td>Tesla</td>\n",
       "      <td>Model S</td>\n",
       "      <td>21000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id     company     model     price\n",
       "0  5595c390-a8e9-11f0-80f2-4d621f880c7b  Oldsmobile  Model 6C  135000.0\n",
       "1  558fa910-a8e9-11f0-80f2-4d621f880c7b       Tesla   Model S   20000.0\n",
       "2  55932b80-a8e9-11f0-80f2-4d621f880c7b       Tesla   Model S   21000.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select all data from the view and convert it to Pandas DataFrame\n",
    "spark.sql(\"select * from my_first_table_view\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a051ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19132916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session terminated successfully\n"
     ]
    }
   ],
   "source": [
    "# Stop Spark session\n",
    "try:\n",
    "    spark.stop()\n",
    "    print('Spark session terminated successfully')\n",
    "except ConnectionRefusedError:\n",
    "    print(\"Spark session already stopped.\")\n",
    "except NameError:\n",
    "    print('Spark session is not defined')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IND320_SMAA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
