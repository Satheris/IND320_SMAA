{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "481b9274",
   "metadata": {},
   "source": [
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c166c2",
   "metadata": {},
   "source": [
    "## AI usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cc00a8",
   "metadata": {},
   "source": [
    "## Log\n",
    "\n",
    "Working on this assignment I am working first on requirements for the Jupyter Notebook and subsequently updating the Streamlit app. \n",
    "\n",
    "I really enjoyed working with the same import structure as in Assignment 2, with the Elhub API. However, I realized that the name I chose for the tables (`elhub_api`) in Cassandra and MongoDB weren't descriptive enough, as we now are using two tables from the same API. I was also unsure of whether the new production data from 2022-2024 should be strictly appended to the old tables or if I could remake the tables with data 2021-2024. Manipulating data with Spark and inserting into Cassandra and MongoDB was otherwise fine, mostly following the same structure from Assignment 2.\n",
    "\n",
    "Updates for the Streamlit app have felt difficult as I haven't caught up with the progress in the lectures. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bf5847",
   "metadata": {},
   "source": [
    "## Links \n",
    "\n",
    "- Github: https://github.com/Satheris/IND320_SMAA\n",
    "- Streamlit app: https://ind320smaa-2eg32uba6uhmrknkwtxzar.streamlit.app/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3451caf4",
   "metadata": {},
   "source": [
    "## Coding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56641cee",
   "metadata": {},
   "source": [
    "### Imports and system variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380122f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import streamlit as st\n",
    "import pymongo\n",
    "from cassandra.cluster import Cluster\n",
    "from pyspark.sql import SparkSession\n",
    "from pyjstat import pyjstat\n",
    "import requests\n",
    "import json\n",
    "import plotly.express as px\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook+pdf+plotly_mimetype\"\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "284061b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables for PySpark (system and version dependent!) \n",
    "# if not already set persistently (e.g., in .bashrc or .bash_profile or Windows environment variables)\n",
    "import os\n",
    "# Set the Java home path to the one you are using ((un)comment and edit as needed):\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Java\\jre1.8.0_471\"\n",
    "\n",
    "# If you are using environments in Python, you can set the environment variables like the alternative below.\n",
    "# The default Python environment is used if the variables are set to \"python\" (edit if needed):\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python\" # or similar to \"/Users/kristian/miniforge3/envs/tf_M1/bin/python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python\" # or similar to \"/Users/kristian/miniforge3/envs/tf_M1/bin/python\"\n",
    "\n",
    "# On Windows you need to specify where the Hadoop drivers are located (uncomment and edit if needed):\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\Users\\saraa\\Documents\\winutils\\hadoop-3.3.1\"\n",
    "\n",
    "# Set the Hadoop version to the one you are using, e.g., none:\n",
    "os.environ[\"PYSPARK_HADOOP_VERSION\"] = \"without\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c07edf8",
   "metadata": {},
   "source": [
    "### Cassandra and Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a799111",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING\tcassandra.cluster:cluster.py:__init__()- Cluster.__init__ called with contact_points specified, but no load_balancing_policy. In the next major version, this will raise an error; please specify a load-balancing policy. (contact_points = ['localhost'], lbp = None)\n",
      "WARNING\tcassandra.cluster:cluster.py:protocol_downgrade()- Downgrading core protocol version from 66 to 65 for ::1:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
      "WARNING\tcassandra.cluster:cluster.py:protocol_downgrade()- Downgrading core protocol version from 65 to 5 for ::1:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n"
     ]
    }
   ],
   "source": [
    "# Connecting to Cassandra\n",
    "# Run local Docker container first\n",
    "cluster = Cluster(['localhost'], port=9042)\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c0a94c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x148c891f410>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up new keyspace\n",
    "#                                              name of keyspace                        replication strategy           replication factor\n",
    "session.execute(\"CREATE KEYSPACE IF NOT EXISTS ind320_keyspace WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\")\n",
    "\n",
    "# assign keyspace\n",
    "session.set_keyspace('ind320_keyspace')\n",
    "\n",
    "# drop tables to start from scratch every time \n",
    "# session.execute(\"DROP TABLE IF EXISTS ind320_keyspace.elhub_api;\")    # table used in Assignment 2\n",
    "session.execute(\"DROP TABLE IF EXISTS ind320_keyspace.elhub_production;\")\n",
    "session.execute(\"DROP TABLE IF EXISTS ind320_keyspace.elhub_consumption;\")\n",
    "\n",
    "# Create new tables \n",
    "session.execute(\"CREATE TABLE IF NOT EXISTS elhub_production \\\n",
    "                (ind int PRIMARY KEY, startTime text, endTime text, lastUpdatedTime text, \\\n",
    "                productionGroup text, quantityKwh float, \\\n",
    "                priceArea text);\")\n",
    "session.execute(\"CREATE TABLE IF NOT EXISTS elhub_consumption \\\n",
    "                (ind int PRIMARY KEY, startTime text, endTime text, lastUpdatedTime text, \\\n",
    "                consumptionGroup text, quantityKwh float, \\\n",
    "                priceArea text, meteringPointCount int)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37f9cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('SparkCassandraApp').\\\n",
    "    config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.5.1').\\\n",
    "    config('spark.cassandra.connection.host', 'localhost').\\\n",
    "    config('spark.sql.extensions', 'com.datastax.spark.connector.CassandraSparkExtensions').\\\n",
    "    config('spark.sql.catalog.mycatalog', 'com.datastax.spark.connector.datasource.CassandraCatalog').\\\n",
    "    config('spark.cassandra.connection.port', '9042').\\\n",
    "    config('spark.driver.host', 'localhost').\\\n",
    "    config('spark.driver.bindAddress', '127.0.0.1').\\\n",
    "    config('spark.sql.adaptive.enabled', 'true').\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4d90f9",
   "metadata": {},
   "source": [
    "#### Testing that the connection works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a4c4c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+\n",
      "|ind| company|  model|\n",
      "+---+--------+-------+\n",
      "|  2|   Tesla|Model 3|\n",
      "|  3|Polestar|      3|\n",
      "|460|    Ford|Transit|\n",
      "|459|    Ford| Escort|\n",
      "|  1|   Tesla|Model S|\n",
      "+---+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# .load() is used to load data from Cassandra table as a Spark DataFrame\n",
    "spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"my_first_table\", keyspace=\"my_first_keyspace\").load().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7376f107",
   "metadata": {},
   "source": [
    "### MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1317cf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "def init_connection():\n",
    "    return pymongo.MongoClient(st.secrets[\"mongo\"][\"uri\"])\n",
    "\n",
    "client = init_connection()\n",
    "\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe80a4c",
   "metadata": {},
   "source": [
    "### Elhub API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53d5415",
   "metadata": {},
   "source": [
    "#### Production data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1405ea61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created DataFrame with 872953 rows\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ind",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "endTime",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "lastUpdatedTime",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "priceArea",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "productionGroup",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "quantityKwh",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "startTime",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "f6d6f8dc-7d77-4b54-b4e0-18092812e241",
       "rows": [
        [
         "0",
         "0",
         "2021-01-01T01:00:00+01:00",
         "2024-12-20T10:35:40+01:00",
         "NO1",
         "hydro",
         "2507716.8",
         "2021-01-01T00:00:00+01:00"
        ],
        [
         "1",
         "1",
         "2021-01-01T02:00:00+01:00",
         "2024-12-20T10:35:40+01:00",
         "NO1",
         "hydro",
         "2494728.0",
         "2021-01-01T01:00:00+01:00"
        ],
        [
         "2",
         "2",
         "2021-01-01T03:00:00+01:00",
         "2024-12-20T10:35:40+01:00",
         "NO1",
         "hydro",
         "2486777.5",
         "2021-01-01T02:00:00+01:00"
        ],
        [
         "3",
         "3",
         "2021-01-01T04:00:00+01:00",
         "2024-12-20T10:35:40+01:00",
         "NO1",
         "hydro",
         "2461176.0",
         "2021-01-01T03:00:00+01:00"
        ],
        [
         "4",
         "4",
         "2021-01-01T05:00:00+01:00",
         "2024-12-20T10:35:40+01:00",
         "NO1",
         "hydro",
         "2466969.2",
         "2021-01-01T04:00:00+01:00"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ind</th>\n",
       "      <th>endTime</th>\n",
       "      <th>lastUpdatedTime</th>\n",
       "      <th>priceArea</th>\n",
       "      <th>productionGroup</th>\n",
       "      <th>quantityKwh</th>\n",
       "      <th>startTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2021-01-01T01:00:00+01:00</td>\n",
       "      <td>2024-12-20T10:35:40+01:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>2507716.8</td>\n",
       "      <td>2021-01-01T00:00:00+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-01-01T02:00:00+01:00</td>\n",
       "      <td>2024-12-20T10:35:40+01:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>2494728.0</td>\n",
       "      <td>2021-01-01T01:00:00+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2021-01-01T03:00:00+01:00</td>\n",
       "      <td>2024-12-20T10:35:40+01:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>2486777.5</td>\n",
       "      <td>2021-01-01T02:00:00+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2021-01-01T04:00:00+01:00</td>\n",
       "      <td>2024-12-20T10:35:40+01:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>2461176.0</td>\n",
       "      <td>2021-01-01T03:00:00+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2021-01-01T05:00:00+01:00</td>\n",
       "      <td>2024-12-20T10:35:40+01:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>2466969.2</td>\n",
       "      <td>2021-01-01T04:00:00+01:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ind                    endTime            lastUpdatedTime priceArea  \\\n",
       "0    0  2021-01-01T01:00:00+01:00  2024-12-20T10:35:40+01:00       NO1   \n",
       "1    1  2021-01-01T02:00:00+01:00  2024-12-20T10:35:40+01:00       NO1   \n",
       "2    2  2021-01-01T03:00:00+01:00  2024-12-20T10:35:40+01:00       NO1   \n",
       "3    3  2021-01-01T04:00:00+01:00  2024-12-20T10:35:40+01:00       NO1   \n",
       "4    4  2021-01-01T05:00:00+01:00  2024-12-20T10:35:40+01:00       NO1   \n",
       "\n",
       "  productionGroup  quantityKwh                  startTime  \n",
       "0           hydro    2507716.8  2021-01-01T00:00:00+01:00  \n",
       "1           hydro    2494728.0  2021-01-01T01:00:00+01:00  \n",
       "2           hydro    2486777.5  2021-01-01T02:00:00+01:00  \n",
       "3           hydro    2461176.0  2021-01-01T03:00:00+01:00  \n",
       "4           hydro    2466969.2  2021-01-01T04:00:00+01:00  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initating lists for traversing the URL\n",
    "years = [2021, 2022, 2023, 2024, 2025]\n",
    "months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "\n",
    "\n",
    "all_records = []\n",
    "\n",
    "for i, year in enumerate(years[:-1]):\n",
    "    for j, month in enumerate(months):\n",
    "\n",
    "        # catching last month with adapted URL for end time\n",
    "        if month == months[-1]:\n",
    "            URL = 'https://api.elhub.no/energy-data/v0/price-areas?dataset=PRODUCTION_PER_GROUP_MBA_HOUR&'\\\n",
    "                f'startDate={year}-{month}-01T00:00:00%2B02:00&endDate={years[i+1]}-{months[0]}-01T00:00:00%2B02:00'\n",
    "            \n",
    "        # all other months follow the same structure\n",
    "        else: \n",
    "            URL = 'https://api.elhub.no/energy-data/v0/price-areas?dataset=PRODUCTION_PER_GROUP_MBA_HOUR&'\\\n",
    "                f'startDate={year}-{month}-01T00:00:00%2B02:00&endDate={year}-{months[j+1]}-01T00:00:00%2B02:00'\n",
    "\n",
    "        payload = { \n",
    "            \"query\": [], \n",
    "            \"response\": { \"format\": \"json-stat2\" } }\n",
    "\n",
    "        response = requests.get(URL, json=payload)\n",
    "        \n",
    "        # print(f\"Y:{year}, m:{month}, Status Code: {response.status_code}\")\n",
    "\n",
    "        data = response.json()\n",
    "\n",
    "        for area in data['data']:\n",
    "            records = area['attributes']['productionPerGroupMbaHour']\n",
    "            for record in records:\n",
    "                record['priceArea'] = area['attributes']['name']\n",
    "                all_records.append(record)\n",
    "\n",
    "elhub_production = pd.DataFrame(all_records)\n",
    "elhub_production.index.name = 'ind'\n",
    "elhub_production = elhub_production.reset_index()\n",
    "\n",
    "print(f\"\\nCreated DataFrame with {len(elhub_production)} rows\")\n",
    "elhub_production.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11902591",
   "metadata": {},
   "source": [
    "#### Consumption data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f56060b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created DataFrame with 876600 rows\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ind",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "consumptionGroup",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "endTime",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "lastUpdatedTime",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "meteringPointCount",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "priceArea",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "quantityKwh",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "startTime",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "2cd3e7cb-a6de-4155-b88b-03cb0a377168",
       "rows": [
        [
         "0",
         "0",
         "cabin",
         "2021-01-01T01:00:00+01:00",
         "2024-12-20T10:35:40+01:00",
         "100607",
         "NO1",
         "177071.56",
         "2021-01-01T00:00:00+01:00"
        ],
        [
         "1",
         "1",
         "cabin",
         "2021-01-01T02:00:00+01:00",
         "2024-12-20T10:35:40+01:00",
         "100607",
         "NO1",
         "171335.12",
         "2021-01-01T01:00:00+01:00"
        ],
        [
         "2",
         "2",
         "cabin",
         "2021-01-01T03:00:00+01:00",
         "2024-12-20T10:35:40+01:00",
         "100607",
         "NO1",
         "164912.02",
         "2021-01-01T02:00:00+01:00"
        ],
        [
         "3",
         "3",
         "cabin",
         "2021-01-01T04:00:00+01:00",
         "2024-12-20T10:35:40+01:00",
         "100607",
         "NO1",
         "160265.77",
         "2021-01-01T03:00:00+01:00"
        ],
        [
         "4",
         "4",
         "cabin",
         "2021-01-01T05:00:00+01:00",
         "2024-12-20T10:35:40+01:00",
         "100607",
         "NO1",
         "159828.69",
         "2021-01-01T04:00:00+01:00"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ind</th>\n",
       "      <th>consumptionGroup</th>\n",
       "      <th>endTime</th>\n",
       "      <th>lastUpdatedTime</th>\n",
       "      <th>meteringPointCount</th>\n",
       "      <th>priceArea</th>\n",
       "      <th>quantityKwh</th>\n",
       "      <th>startTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>cabin</td>\n",
       "      <td>2021-01-01T01:00:00+01:00</td>\n",
       "      <td>2024-12-20T10:35:40+01:00</td>\n",
       "      <td>100607</td>\n",
       "      <td>NO1</td>\n",
       "      <td>177071.56</td>\n",
       "      <td>2021-01-01T00:00:00+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>cabin</td>\n",
       "      <td>2021-01-01T02:00:00+01:00</td>\n",
       "      <td>2024-12-20T10:35:40+01:00</td>\n",
       "      <td>100607</td>\n",
       "      <td>NO1</td>\n",
       "      <td>171335.12</td>\n",
       "      <td>2021-01-01T01:00:00+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>cabin</td>\n",
       "      <td>2021-01-01T03:00:00+01:00</td>\n",
       "      <td>2024-12-20T10:35:40+01:00</td>\n",
       "      <td>100607</td>\n",
       "      <td>NO1</td>\n",
       "      <td>164912.02</td>\n",
       "      <td>2021-01-01T02:00:00+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>cabin</td>\n",
       "      <td>2021-01-01T04:00:00+01:00</td>\n",
       "      <td>2024-12-20T10:35:40+01:00</td>\n",
       "      <td>100607</td>\n",
       "      <td>NO1</td>\n",
       "      <td>160265.77</td>\n",
       "      <td>2021-01-01T03:00:00+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>cabin</td>\n",
       "      <td>2021-01-01T05:00:00+01:00</td>\n",
       "      <td>2024-12-20T10:35:40+01:00</td>\n",
       "      <td>100607</td>\n",
       "      <td>NO1</td>\n",
       "      <td>159828.69</td>\n",
       "      <td>2021-01-01T04:00:00+01:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ind consumptionGroup                    endTime            lastUpdatedTime  \\\n",
       "0    0            cabin  2021-01-01T01:00:00+01:00  2024-12-20T10:35:40+01:00   \n",
       "1    1            cabin  2021-01-01T02:00:00+01:00  2024-12-20T10:35:40+01:00   \n",
       "2    2            cabin  2021-01-01T03:00:00+01:00  2024-12-20T10:35:40+01:00   \n",
       "3    3            cabin  2021-01-01T04:00:00+01:00  2024-12-20T10:35:40+01:00   \n",
       "4    4            cabin  2021-01-01T05:00:00+01:00  2024-12-20T10:35:40+01:00   \n",
       "\n",
       "   meteringPointCount priceArea  quantityKwh                  startTime  \n",
       "0              100607       NO1    177071.56  2021-01-01T00:00:00+01:00  \n",
       "1              100607       NO1    171335.12  2021-01-01T01:00:00+01:00  \n",
       "2              100607       NO1    164912.02  2021-01-01T02:00:00+01:00  \n",
       "3              100607       NO1    160265.77  2021-01-01T03:00:00+01:00  \n",
       "4              100607       NO1    159828.69  2021-01-01T04:00:00+01:00  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initating lists for traversing the URL\n",
    "years = [2021, 2022, 2023, 2024, 2025]\n",
    "months = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n",
    "\n",
    "\n",
    "all_records = []\n",
    "\n",
    "for i, year in enumerate(years[:-1]):\n",
    "    for j, month in enumerate(months):\n",
    "        \n",
    "        # catching last month with adapted URL for end time\n",
    "        if month == months[-1]:\n",
    "            URL = 'https://api.elhub.no/energy-data/v0/price-areas?dataset=CONSUMPTION_PER_GROUP_MBA_HOUR&'\\\n",
    "                f'startDate={year}-{month}-01T00:00:00%2B02:00&endDate={years[i+1]}-{months[0]}-01T00:00:00%2B02:00'\n",
    "        \n",
    "        # all other months follow the same structure\n",
    "        else: \n",
    "            URL = 'https://api.elhub.no/energy-data/v0/price-areas?dataset=CONSUMPTION_PER_GROUP_MBA_HOUR&'\\\n",
    "                f'startDate={year}-{month}-01T00:00:00%2B02:00&endDate={year}-{months[j+1]}-01T00:00:00%2B02:00'\n",
    "\n",
    "        payload = { \n",
    "            \"query\": [], \n",
    "            \"response\": { \"format\": \"json-stat2\" } }\n",
    "\n",
    "        response = requests.get(URL, json=payload)\n",
    "        \n",
    "        # print(f\"Y:{year}, m:{month}, Status Code: {response.status_code}\")\n",
    "\n",
    "        data = response.json()\n",
    "\n",
    "        for area in data['data']:\n",
    "            records = area['attributes']['consumptionPerGroupMbaHour']\n",
    "            for record in records:\n",
    "                record['priceArea'] = area['attributes']['name']\n",
    "                all_records.append(record)\n",
    "\n",
    "elhub_consumption = pd.DataFrame(all_records)\n",
    "elhub_consumption.index.name = 'ind'\n",
    "elhub_consumption = elhub_consumption.reset_index()\n",
    "\n",
    "print(f\"\\nCreated DataFrame with {len(elhub_consumption)} rows\")\n",
    "elhub_consumption.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32852405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved elhub_production to Cassandra\n",
      "Saved elhub_consumption to Cassandra\n"
     ]
    }
   ],
   "source": [
    "# I found that the columns in the Cassandra table was constructed with lowercase letters.\n",
    "# Therefore, I need to convert the column names to lowercase before writing to Cassandra\n",
    "\n",
    "str_list = ['elhub_production', 'elhub_consumption']\n",
    "\n",
    "for i, table in enumerate([elhub_production, elhub_consumption]):\n",
    "    name_dict = {}\n",
    "    for capitalname in (table.columns):\n",
    "        name_dict[capitalname] = capitalname.lower()\n",
    "    \n",
    "    table = table.rename(columns=name_dict)\n",
    "\n",
    "    # Convert the Pandas DataFrame to Spark DataFrame and save it to Cassandra (append mode)\n",
    "    spark.createDataFrame(table).write.format('org.apache.spark.sql.cassandra')\\\n",
    "    .options(table=str_list[i], keyspace='ind320_keyspace').mode('append').save()\n",
    "\n",
    "    print(f'Saved {str_list[i]} to Cassandra')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a540c0",
   "metadata": {},
   "source": [
    "### Saving dfs to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44431732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully inserted elhub_production to MongoDB\n",
      "Successfully inserted elhub_consumption to MongoDB\n"
     ]
    }
   ],
   "source": [
    "# Selecting a database and a collection\n",
    "database = client['project']\n",
    "\n",
    "for table in str_list:\n",
    "    collection = database[table]\n",
    "    collection.delete_many({}) # starting fresh\n",
    "\n",
    "    spark.read.format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .options(table=table, keyspace=\"ind320_keyspace\").load()\\\n",
    "    .createOrReplaceTempView(f\"{table}_view\")\n",
    "\n",
    "    df_spark = spark.sql(f\"SELECT priceArea, {table.split('_')[1]}Group, startTime, quantityKwh FROM {table}_view\")\n",
    "\n",
    "    # Convert DataFrame to JSON and dumping to MongoDB\n",
    "    df_pd = df_spark.toPandas()\n",
    "    json_data = df_pd.to_json(orient='records')\n",
    "\n",
    "    documents = json.loads(json_data)\n",
    "    try: \n",
    "        collection.insert_many(documents)\n",
    "        print(f'Successfully inserted {table} to MongoDB')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523e10e9",
   "metadata": {},
   "source": [
    "### Stopping Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "440971e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session terminated successfully\n"
     ]
    }
   ],
   "source": [
    "# Stop Spark session\n",
    "try:\n",
    "    spark.stop()\n",
    "    print('Spark session terminated successfully')\n",
    "except ConnectionRefusedError:\n",
    "    print(\"Spark session already stopped.\")\n",
    "except NameError:\n",
    "    print('Spark session is not defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b83cc53",
   "metadata": {},
   "source": [
    "### Testing for Streamlit app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8e35a6",
   "metadata": {},
   "source": [
    "#### Plotly map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbb9cbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "\n",
    "# with open(r'C:\\Users\\saraa\\Documents\\IND320_SMAA\\project\\data\\file.geojson') as file: \n",
    "#     priceAreas = json.load(file)\n",
    "\n",
    "# fig = px.choropleth(geojson=priceAreas,\n",
    "#                     locations='fips',\n",
    "#                     featureidkey='properties.id',\n",
    "#                     )\n",
    "\n",
    "# fig.update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\n",
    "# fig.update_geos(fitbounds=\"locations\", visible=True)\n",
    "\n",
    "\n",
    "# # # Create figure\n",
    "# # fig = go.Figure()\n",
    "\n",
    "# # # Add choropleth mapbox (even without data values, just for outlines)\n",
    "# # fig.add_trace(go.Choroplethmap(\n",
    "# #     geojson=priceAreas,\n",
    "# #     locations=[],  # Empty since we just want outlines\n",
    "# #     featureidkey=\"properties.id\",  # Adjust based on your GeoJSON structure\n",
    "# #     z=[],  # Empty data\n",
    "# #     colorscale=\"Blues\",  # Doesn't matter since no data\n",
    "# #     showscale=False,  # Hide color scale\n",
    "# #     marker_line_width=2,  # Outline width\n",
    "# #     marker_opacity=0,  # Transparent fill\n",
    "# #     marker_line_color=\"black\"  # Outline color\n",
    "# # ))\n",
    "\n",
    "# # # Update layout with mapbox style\n",
    "# # fig.update_layout(\n",
    "# #     mapbox_style=\"open-street-map\",\n",
    "# #     mapbox_zoom=10,\n",
    "# #     mapbox_center={\"lat\": 40.7, \"lat\": -73.9},  # Adjust to your area\n",
    "# #     margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0}\n",
    "# # )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5b6eaa",
   "metadata": {},
   "source": [
    "#### Snow drift "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "85a0924b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Yearly average snow drift (Qt) per season:\n",
      "Overall average Qt over all seasons: 14.1 tonnes/m\n",
      "\n",
      "Yearly average snow drift (Qt) per season (in tonnes/m) and control type:\n",
      "   season Qt (tonnes/m)         Control\n",
      "2021-2022          14.1 Wind controlled\n",
      "\n",
      "Overall average Qt over all seasons: 14.1 tonnes/m\n",
      "\n",
      "Necessary fence heights per season (in meters):\n",
      "   season Wyoming (m) Slat-and-wire (m) Solid (m)\n",
      "2021-2022         1.3               1.3       2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saraa\\Documents\\IND320_SMAA\\project\\utils\\snowdrift.py:200: UserWarning:\n",
      "\n",
      "FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.snowdrift import snowdrift_plot\n",
    "from utils.common import area_to_geoplacement\n",
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "\n",
    "# from utils/common.py\n",
    "# modified to download July-June time period\n",
    "def openmeteo_download_snowdrift(area, startYear=2021, endYear=2022) -> pd.DataFrame:\n",
    "    # Setup the Open-Meteo API client with cache and retry on error\n",
    "    cache_session = requests_cache.CachedSession('.cache', expire_after = 3600)\n",
    "    retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "    openmeteo = openmeteo_requests.Client(session = retry_session)\n",
    "\n",
    "    longitude, latitude = area_to_geoplacement(area)\n",
    "\n",
    "    # Make sure all required weather variables are listed here\n",
    "    # The order of variables in hourly or daily is important to assign them correctly below\n",
    "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    params = {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"start_date\": f\"{startYear}-07-01\",\n",
    "        \"end_date\": f\"{endYear}-06-30\",\n",
    "        \"hourly\": [\"temperature_2m\", \"precipitation\", \"wind_speed_10m\", \"wind_direction_10m\", \"wind_gusts_10m\"],\n",
    "        \"models\": \"era5\",\n",
    "        \"timezone\": \"Europe/Berlin\",\n",
    "        \"wind_speed_unit\": \"ms\"}\n",
    "    \n",
    "    responses = openmeteo.weather_api(url, params=params)\n",
    "    response = responses[0]    \n",
    "\n",
    "    # Process hourly data. The order of variables needs to be the same as requested.\n",
    "    hourly = response.Hourly()\n",
    "    hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\n",
    "    hourly_wind_direction_10m = hourly.Variables(1).ValuesAsNumpy()\n",
    "    hourly_wind_speed_10m = hourly.Variables(2).ValuesAsNumpy()\n",
    "    hourly_wind_gusts_10m = hourly.Variables(3).ValuesAsNumpy()\n",
    "    hourly_precipitation = hourly.Variables(4).ValuesAsNumpy()\n",
    "\n",
    "    hourly_data = {\"time\": pd.date_range(\n",
    "        start = pd.to_datetime(hourly.Time(), unit = \"s\", utc = True), # .tz_convert('Europe/Oslo'),\n",
    "        end =  pd.to_datetime(hourly.TimeEnd(), unit = \"s\", utc = True), # .tz_convert('Europe/Oslo'),\n",
    "        freq = pd.Timedelta(seconds = hourly.Interval()),\n",
    "        inclusive = \"left\")}\n",
    "    \n",
    "    hourly_data['time'] = pd.to_datetime(hourly_data['time']).tz_localize(None) + pd.Timedelta(hours=1)\n",
    "\n",
    "    hourly_data[\"temperature_2m (°C)\"] = hourly_temperature_2m\n",
    "    hourly_data[\"wind_direction_10m (°)\"] = hourly_wind_direction_10m\n",
    "    hourly_data[\"wind_speed_10m (m/s)\"] = hourly_wind_speed_10m\n",
    "    hourly_data[\"wind_gusts_10m (m/s)\"] = hourly_wind_gusts_10m\n",
    "    hourly_data[\"precipitation (mm)\"] = hourly_precipitation\n",
    "\n",
    "    df = pd.DataFrame(data = hourly_data)\n",
    "\n",
    "    return df\n",
    "\n",
    "# openmeteo_download_snowdrift('NO1')\n",
    "\n",
    "snowdrift_plot(openmeteo_download_snowdrift('NO1', endYear=2022))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b8c13f",
   "metadata": {},
   "source": [
    "#### Meteorology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd44ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7bd3530b",
   "metadata": {},
   "source": [
    "#### Forecasting energy data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131fbfef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff11869",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IND320_SMAA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
