{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "481b9274",
   "metadata": {},
   "source": [
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c166c2",
   "metadata": {},
   "source": [
    "## AI usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cc00a8",
   "metadata": {},
   "source": [
    "## Log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bf5847",
   "metadata": {},
   "source": [
    "## Links \n",
    "\n",
    "- Github: https://github.com/Satheris/IND320_SMAA\n",
    "- Streamlit app: https://ind320smaa-2eg32uba6uhmrknkwtxzar.streamlit.app/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3451caf4",
   "metadata": {},
   "source": [
    "## Coding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56641cee",
   "metadata": {},
   "source": [
    "### Imports and system variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "380122f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import streamlit as st\n",
    "import pymongo\n",
    "from cassandra.cluster import Cluster\n",
    "from pyspark.sql import SparkSession\n",
    "from pyjstat import pyjstat\n",
    "import requests\n",
    "import json\n",
    "import plotly.express as px\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook+pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables for PySpark (system and version dependent!) \n",
    "# if not already set persistently (e.g., in .bashrc or .bash_profile or Windows environment variables)\n",
    "import os\n",
    "# Set the Java home path to the one you are using ((un)comment and edit as needed):\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Java\\jre1.8.0_471\"\n",
    "\n",
    "# If you are using environments in Python, you can set the environment variables like the alternative below.\n",
    "# The default Python environment is used if the variables are set to \"python\" (edit if needed):\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python\" # or similar to \"/Users/kristian/miniforge3/envs/tf_M1/bin/python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python\" # or similar to \"/Users/kristian/miniforge3/envs/tf_M1/bin/python\"\n",
    "\n",
    "# On Windows you need to specify where the Hadoop drivers are located (uncomment and edit if needed):\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\Users\\saraa\\Documents\\winutils\\hadoop-3.3.1\"\n",
    "\n",
    "# Set the Hadoop version to the one you are using, e.g., none:\n",
    "os.environ[\"PYSPARK_HADOOP_VERSION\"] = \"without\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c07edf8",
   "metadata": {},
   "source": [
    "### Cassandra and Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING\tcassandra.cluster:cluster.py:__init__()- Cluster.__init__ called with contact_points specified, but no load_balancing_policy. In the next major version, this will raise an error; please specify a load-balancing policy. (contact_points = ['localhost'], lbp = None)\n",
      "WARNING\tcassandra.cluster:cluster.py:protocol_downgrade()- Downgrading core protocol version from 66 to 65 for ::1:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
      "WARNING\tcassandra.cluster:cluster.py:protocol_downgrade()- Downgrading core protocol version from 65 to 5 for ::1:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n"
     ]
    }
   ],
   "source": [
    "# Connecting to Cassandra\n",
    "# Run local Docker container first\n",
    "cluster = Cluster(['localhost'], port=9042)\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c0a94c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x24e6e517350>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up new keyspace\n",
    "#                                              name of keyspace                        replication strategy           replication factor\n",
    "session.execute(\"CREATE KEYSPACE IF NOT EXISTS ind320_keyspace WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\")\n",
    "\n",
    "# Create a new table\n",
    "session.set_keyspace('ind320_keyspace')\n",
    "# session.execute(\"DROP TABLE IF EXISTS ind320_keyspace.elhub_api;\") # Starting from scratch every time\n",
    "session.execute(\"CREATE TABLE IF NOT EXISTS elhub_api (ind int PRIMARY KEY, endTime text, lastUpdatedTime text, priceArea text, productionGroup text, quantityKwh float, startTime text);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37f9cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('SparkCassandraApp').\\\n",
    "    config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.5.1').\\\n",
    "    config('spark.cassandra.connection.host', 'localhost').\\\n",
    "    config('spark.sql.extensions', 'com.datastax.spark.connector.CassandraSparkExtensions').\\\n",
    "    config('spark.sql.catalog.mycatalog', 'com.datastax.spark.connector.datasource.CassandraCatalog').\\\n",
    "    config('spark.cassandra.connection.port', '9042').\\\n",
    "    config('spark.driver.host', 'localhost').\\\n",
    "    config('spark.driver.bindAddress', '127.0.0.1').\\\n",
    "    config('spark.sql.adaptive.enabled', 'true').\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4d90f9",
   "metadata": {},
   "source": [
    "#### Testing that the connection works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a4c4c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+\n",
      "|ind| company|  model|\n",
      "+---+--------+-------+\n",
      "|  2|   Tesla|Model 3|\n",
      "|  3|Polestar|      3|\n",
      "|460|    Ford|Transit|\n",
      "|459|    Ford| Escort|\n",
      "|  1|   Tesla|Model S|\n",
      "+---+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# .load() is used to load data from Cassandra table as a Spark DataFrame\n",
    "spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"my_first_table\", keyspace=\"my_first_keyspace\").load().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7376f107",
   "metadata": {},
   "source": [
    "### MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1317cf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "def init_connection():\n",
    "    return pymongo.MongoClient(st.secrets[\"mongo\"][\"uri\"])\n",
    "\n",
    "client = init_connection()\n",
    "\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a3192d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IND320_SMAA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
