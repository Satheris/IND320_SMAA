{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "481b9274",
   "metadata": {},
   "source": [
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c166c2",
   "metadata": {},
   "source": [
    "## AI usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cc00a8",
   "metadata": {},
   "source": [
    "## Log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bf5847",
   "metadata": {},
   "source": [
    "## Links \n",
    "\n",
    "- Github: https://github.com/Satheris/IND320_SMAA\n",
    "- Streamlit app: https://ind320smaa-2eg32uba6uhmrknkwtxzar.streamlit.app/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3451caf4",
   "metadata": {},
   "source": [
    "## Coding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56641cee",
   "metadata": {},
   "source": [
    "### Imports and system variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "380122f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import streamlit as st\n",
    "import pymongo\n",
    "from cassandra.cluster import Cluster\n",
    "from pyspark.sql import SparkSession\n",
    "from pyjstat import pyjstat\n",
    "import requests\n",
    "import json\n",
    "import plotly.express as px\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook+pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables for PySpark (system and version dependent!) \n",
    "# if not already set persistently (e.g., in .bashrc or .bash_profile or Windows environment variables)\n",
    "import os\n",
    "# Set the Java home path to the one you are using ((un)comment and edit as needed):\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Java\\jre1.8.0_471\"\n",
    "\n",
    "# If you are using environments in Python, you can set the environment variables like the alternative below.\n",
    "# The default Python environment is used if the variables are set to \"python\" (edit if needed):\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python\" # or similar to \"/Users/kristian/miniforge3/envs/tf_M1/bin/python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python\" # or similar to \"/Users/kristian/miniforge3/envs/tf_M1/bin/python\"\n",
    "\n",
    "# On Windows you need to specify where the Hadoop drivers are located (uncomment and edit if needed):\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\Users\\saraa\\Documents\\winutils\\hadoop-3.3.1\"\n",
    "\n",
    "# Set the Hadoop version to the one you are using, e.g., none:\n",
    "os.environ[\"PYSPARK_HADOOP_VERSION\"] = \"without\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c07edf8",
   "metadata": {},
   "source": [
    "### Cassandra and Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING\tcassandra.cluster:cluster.py:__init__()- Cluster.__init__ called with contact_points specified, but no load_balancing_policy. In the next major version, this will raise an error; please specify a load-balancing policy. (contact_points = ['localhost'], lbp = None)\n",
      "WARNING\tcassandra.cluster:cluster.py:protocol_downgrade()- Downgrading core protocol version from 66 to 65 for ::1:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
      "WARNING\tcassandra.cluster:cluster.py:protocol_downgrade()- Downgrading core protocol version from 65 to 5 for ::1:9042. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n"
     ]
    }
   ],
   "source": [
    "# Connecting to Cassandra\n",
    "# Run local Docker container first\n",
    "cluster = Cluster(['localhost'], port=9042)\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c0a94c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x24e6e517350>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up new keyspace\n",
    "#                                              name of keyspace                        replication strategy           replication factor\n",
    "session.execute(\"CREATE KEYSPACE IF NOT EXISTS ind320_keyspace WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\")\n",
    "\n",
    "# Create a new table\n",
    "session.set_keyspace('ind320_keyspace')\n",
    "# session.execute(\"DROP TABLE IF EXISTS ind320_keyspace.elhub_api;\") # Starting from scratch every time\n",
    "session.execute(\"CREATE TABLE IF NOT EXISTS elhub_api (ind int PRIMARY KEY, endTime text, lastUpdatedTime text, priceArea text, productionGroup text, quantityKwh float, startTime text);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37f9cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('SparkCassandraApp').\\\n",
    "    config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.5.1').\\\n",
    "    config('spark.cassandra.connection.host', 'localhost').\\\n",
    "    config('spark.sql.extensions', 'com.datastax.spark.connector.CassandraSparkExtensions').\\\n",
    "    config('spark.sql.catalog.mycatalog', 'com.datastax.spark.connector.datasource.CassandraCatalog').\\\n",
    "    config('spark.cassandra.connection.port', '9042').\\\n",
    "    config('spark.driver.host', 'localhost').\\\n",
    "    config('spark.driver.bindAddress', '127.0.0.1').\\\n",
    "    config('spark.sql.adaptive.enabled', 'true').\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4d90f9",
   "metadata": {},
   "source": [
    "#### Testing that the connection works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a4c4c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+\n",
      "|ind| company|  model|\n",
      "+---+--------+-------+\n",
      "|  2|   Tesla|Model 3|\n",
      "|  3|Polestar|      3|\n",
      "|460|    Ford|Transit|\n",
      "|459|    Ford| Escort|\n",
      "|  1|   Tesla|Model S|\n",
      "+---+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# .load() is used to load data from Cassandra table as a Spark DataFrame\n",
    "spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"my_first_table\", keyspace=\"my_first_keyspace\").load().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7376f107",
   "metadata": {},
   "source": [
    "### MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1317cf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "def init_connection():\n",
    "    return pymongo.MongoClient(st.secrets[\"mongo\"][\"uri\"])\n",
    "\n",
    "client = init_connection()\n",
    "\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe80a4c",
   "metadata": {},
   "source": [
    "### Elhub API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26a3192d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response saved to 'api_response_assign4.json'\n",
      "\n",
      "Status Code: 200\n",
      "Headers: application/json; charset=utf-8\n",
      "\n",
      "Created DataFrame with 17856 rows\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "endTime",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "lastUpdatedTime",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "priceArea",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "productionGroup",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "quantityKwh",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "startTime",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "f247afcb-2350-4441-8ec2-585dbf5d727d",
       "rows": [
        [
         "0",
         "2021-01-01T01:00:00+01:00",
         "2024-12-20T10:35:40+01:00",
         "NO1",
         "hydro",
         "2507716.8",
         "2021-01-01T00:00:00+01:00"
        ],
        [
         "1",
         "2021-01-01T02:00:00+01:00",
         "2024-12-20T10:35:40+01:00",
         "NO1",
         "hydro",
         "2494728.0",
         "2021-01-01T01:00:00+01:00"
        ],
        [
         "2",
         "2021-01-01T03:00:00+01:00",
         "2024-12-20T10:35:40+01:00",
         "NO1",
         "hydro",
         "2486777.5",
         "2021-01-01T02:00:00+01:00"
        ],
        [
         "3",
         "2021-01-01T04:00:00+01:00",
         "2024-12-20T10:35:40+01:00",
         "NO1",
         "hydro",
         "2461176.0",
         "2021-01-01T03:00:00+01:00"
        ],
        [
         "4",
         "2021-01-01T05:00:00+01:00",
         "2024-12-20T10:35:40+01:00",
         "NO1",
         "hydro",
         "2466969.2",
         "2021-01-01T04:00:00+01:00"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endTime</th>\n",
       "      <th>lastUpdatedTime</th>\n",
       "      <th>priceArea</th>\n",
       "      <th>productionGroup</th>\n",
       "      <th>quantityKwh</th>\n",
       "      <th>startTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-01T01:00:00+01:00</td>\n",
       "      <td>2024-12-20T10:35:40+01:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>2507716.8</td>\n",
       "      <td>2021-01-01T00:00:00+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-01T02:00:00+01:00</td>\n",
       "      <td>2024-12-20T10:35:40+01:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>2494728.0</td>\n",
       "      <td>2021-01-01T01:00:00+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-01T03:00:00+01:00</td>\n",
       "      <td>2024-12-20T10:35:40+01:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>2486777.5</td>\n",
       "      <td>2021-01-01T02:00:00+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-01T04:00:00+01:00</td>\n",
       "      <td>2024-12-20T10:35:40+01:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>2461176.0</td>\n",
       "      <td>2021-01-01T03:00:00+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-01T05:00:00+01:00</td>\n",
       "      <td>2024-12-20T10:35:40+01:00</td>\n",
       "      <td>NO1</td>\n",
       "      <td>hydro</td>\n",
       "      <td>2466969.2</td>\n",
       "      <td>2021-01-01T04:00:00+01:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     endTime            lastUpdatedTime priceArea  \\\n",
       "0  2021-01-01T01:00:00+01:00  2024-12-20T10:35:40+01:00       NO1   \n",
       "1  2021-01-01T02:00:00+01:00  2024-12-20T10:35:40+01:00       NO1   \n",
       "2  2021-01-01T03:00:00+01:00  2024-12-20T10:35:40+01:00       NO1   \n",
       "3  2021-01-01T04:00:00+01:00  2024-12-20T10:35:40+01:00       NO1   \n",
       "4  2021-01-01T05:00:00+01:00  2024-12-20T10:35:40+01:00       NO1   \n",
       "\n",
       "  productionGroup  quantityKwh                  startTime  \n",
       "0           hydro    2507716.8  2021-01-01T00:00:00+01:00  \n",
       "1           hydro    2494728.0  2021-01-01T01:00:00+01:00  \n",
       "2           hydro    2486777.5  2021-01-01T02:00:00+01:00  \n",
       "3           hydro    2461176.0  2021-01-01T03:00:00+01:00  \n",
       "4           hydro    2466969.2  2021-01-01T04:00:00+01:00  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL = 'https://api.elhub.no/energy-data/v0/price-areas?dataset=PRODUCTION_PER_GROUP_MBA_HOUR' \\\n",
    "        '&startDate=2021-01-01T00:00:00%2B02:00&endDate=2021-02-01T00:00:00%2B02:00'\n",
    "\n",
    "payload = { \n",
    "    \"query\": [], \n",
    "    \"response\": { \"format\": \"json-stat2\" } }\n",
    "\n",
    "response = requests.get(URL, json=payload)\n",
    "data = response.json()\n",
    "\n",
    "# Writing the data into a file\n",
    "with open(r'data\\api_response_assign4.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(response.json(), f, indent=2, ensure_ascii=False)\n",
    "print(\"Response saved to 'api_response_assign4.json'\")\n",
    "\n",
    "\n",
    "# Prints for status\n",
    "print(\"\\nStatus Code:\", response.status_code)\n",
    "print(\"Headers:\", response.headers.get('content-type'))\n",
    "\n",
    "\n",
    "# Extract all production records\n",
    "all_records = []\n",
    "for area in data['data']:\n",
    "    records = area['attributes']['productionPerGroupMbaHour']\n",
    "    for record in records:\n",
    "        record['priceArea'] = area['attributes']['name']  # Add area name\n",
    "        all_records.append(record)\n",
    "\n",
    "df = pd.DataFrame(all_records)\n",
    "print(f\"\\nCreated DataFrame with {len(df)} rows\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a540c0",
   "metadata": {},
   "source": [
    "### Saving df to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44431732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a database and a collection\n",
    "database = client['project']\n",
    "collection = database['data']\n",
    "collection.delete_many({}) # starting fresh\n",
    "\n",
    "# Convert DataFrame to JSON and dumping to MongoDB\n",
    "df_pd = df_spark.toPandas()\n",
    "json_data = df_pd.to_json(orient='records')\n",
    "\n",
    "documents = json.loads(json_data)\n",
    "try: \n",
    "    collection.insert_many(documents)\n",
    "    print('Successfully inserted data to MongoDB')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523e10e9",
   "metadata": {},
   "source": [
    "### Stopping Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440971e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "try:\n",
    "    spark.stop()\n",
    "    print('Spark session terminated successfully')\n",
    "except ConnectionRefusedError:\n",
    "    print(\"Spark session already stopped.\")\n",
    "except NameError:\n",
    "    print('Spark session is not defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b83cc53",
   "metadata": {},
   "source": [
    "### Testing for Streamlit app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb9cbc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IND320_SMAA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
