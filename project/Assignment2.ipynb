{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9606477",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d72be5",
   "metadata": {},
   "source": [
    "## AI usage \n",
    "\n",
    "Used DeepSeek for help on how to connect Cassandra and Spark. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e9c323",
   "metadata": {},
   "source": [
    "## Log \n",
    "\n",
    "For this assignment I will try to work more systematic than the last. I will focus on finishing all the elements for the notebook first, and work on the streamlit app afterwards. \n",
    "\n",
    "I started by connecting Cassandra and Spark, but ran into problems regarding the sparksession. After troubleshooting for a while and getting some help, we ended up using DeepSeek for help on how to maybe fix it. DeepSeek suggested adding the last three lines of code to force sessionbuilder to use the localhost. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c299533c",
   "metadata": {},
   "source": [
    "## Links \n",
    "\n",
    "- Github: https://github.com/Satheris/IND320_SMAA\n",
    "- Streamlit app: https://ind320smaa-2eg32uba6uhmrknkwtxzar.streamlit.app/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f530005e",
   "metadata": {},
   "source": [
    "## Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2d9397",
   "metadata": {},
   "source": [
    "### Imports and system variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d5b8e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46f781e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables for PySpark (system and version dependent!) \n",
    "# if not already set persistently (e.g., in .bashrc or .bash_profile or Windows environment variables)\n",
    "import os\n",
    "# Set the Java home path to the one you are using ((un)comment and edit as needed):\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Java\\jre1.8.0_471\"\n",
    "\n",
    "# If you are using environments in Python, you can set the environment variables like the alternative below.\n",
    "# The default Python environment is used if the variables are set to \"python\" (edit if needed):\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python\" # or similar to \"/Users/kristian/miniforge3/envs/tf_M1/bin/python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python\" # or similar to \"/Users/kristian/miniforge3/envs/tf_M1/bin/python\"\n",
    "\n",
    "# On Windows you need to specify where the Hadoop drivers are located (uncomment and edit if needed):\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\Users\\saraa\\Documents\\winutils\\hadoop-3.3.1\"\n",
    "\n",
    "# Set the Hadoop version to the one you are using, e.g., none:\n",
    "os.environ[\"PYSPARK_HADOOP_VERSION\"] = \"without\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa36443",
   "metadata": {},
   "source": [
    "### Cassandra and Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbdc9774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to Cassandra\n",
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster(['localhost'], port=9042)\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec95e2a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x21f7ab32850>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up new keyspace (first time only)\n",
    "#                                              name of keyspace                        replication strategy           replication factor\n",
    "session.execute(\"CREATE KEYSPACE IF NOT EXISTS my_first_keyspace WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "594220f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkCassandraApp').\\\n",
    "    config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.5.1').\\\n",
    "    config('spark.cassandra.connection.host', 'localhost').\\\n",
    "    config('spark.sql.extensions', 'com.datastax.spark.connector.CassandraSparkExtensions').\\\n",
    "    config('spark.sql.catalog.mycatalog', 'com.datastax.spark.connector.datasource.CassandraCatalog').\\\n",
    "    config('spark.cassandra.connection.port', '9042').\\\n",
    "    config('spark.driver.host', 'localhost').\\\n",
    "    config('spark.driver.bindAddress', '127.0.0.1').\\\n",
    "    config('spark.sql.adaptive.enabled', 'true').\\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96b67059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+\n",
      "|ind| company|  model|\n",
      "+---+--------+-------+\n",
      "|  1|   Tesla|Model S|\n",
      "|  3|Polestar|      3|\n",
      "|  2|   Tesla|Model 3|\n",
      "+---+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# .load() is used to load data from Cassandra table as a Spark DataFrame\n",
    "spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"my_first_table\", keyspace=\"my_first_keyspace\").load().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a38c3847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create view for simpler SQL queries\n",
    "spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"table_with_uuid\", keyspace=\"my_first_keyspace\").load().createOrReplaceTempView(\"my_first_table_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f7c48da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+\n",
      "| planet| distance| diameter|\n",
      "+-------+---------+---------+\n",
      "|Mercury| 0.387 AU|  4878 km|\n",
      "|  Venus| 0.723 AU| 12104 km|\n",
      "|  Earth| 1.000 AU| 12756 km|\n",
      "|   Mars| 1.524 AU|  6787 km|\n",
      "|Jupiter| 5.203 AU|142796 km|\n",
      "| Saturn| 9.546 AU|120660 km|\n",
      "| Uranus|19.218 AU| 51118 km|\n",
      "|Neptune|30.069 AU| 48600 km|\n",
      "+-------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read CSV file into Spark DataFrame\n",
    "planets = spark.read.csv(\"../data/planets.csv\", header=True, inferSchema=True)\n",
    "planets.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a89b09",
   "metadata": {},
   "source": [
    "### MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1f4a17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1339124d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dfe96e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "151ae79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session terminated successfully\n"
     ]
    }
   ],
   "source": [
    "# Stop Spark session\n",
    "try:\n",
    "    spark.stop()\n",
    "    print('Spark session terminated successfully')\n",
    "except ConnectionRefusedError:\n",
    "    print(\"Spark session already stopped.\")\n",
    "except NameError:\n",
    "    print('Spark session is not defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b66397",
   "metadata": {},
   "source": [
    "# General\n",
    "\n",
    "\n",
    "A Streamlit app running from https://[yourproject].streamlit.app/.\n",
    "This is an online version of the project, accessing data that has been exported to CSV format and accessing your MongoDB database for additional data.\n",
    "The code, hosted at GitHub, must include relevant comments from the Jupyter Notebook and further comments regarding Streamlit usage.\n",
    "\n",
    "\n",
    "## Tasks\n",
    "\n",
    "### Local database: Cassandra\n",
    "If not already done, set up Cassandra and Spark as described in the book.\n",
    "Test that your Spark-Cassandra connection works.\n",
    "The Cassandra database will be accessed from the Jupyter Notebook and used to store data from the API mentioned later. \n",
    "\n",
    "### Remote database: MongoDB\n",
    "If not already done, prepare a MongoDB account at mongodb.com.\n",
    "Test that you can manipulate data from Python.\n",
    "The MongoDB database will store data that has been trimmed/curated/prepared through the Jupyter Notebook and Spark filtering.\n",
    "These data will be accessed directly from the Streamlit app.\n",
    "\n",
    "### API\n",
    "Familiarise yourself with the API connection at https://api.elhub.noLenker til en ekstern side.\n",
    "\n",
    "Observe how time is encoded and how transitions between summer and winter time are handled.\n",
    "Be aware of the time period limitations for each API request and how this differs between datasets.\n",
    "\n",
    "\n",
    "### Jupyter Notebook\n",
    "\n",
    "#### Standard requirements\n",
    "\n",
    "Must include a brief description of AI usage.\n",
    "\n",
    "Must include a 300-500-word log describing the compulsory work (including both Jupyter Notebook and Streamlit experience).\n",
    "\n",
    "Must include links to your public GitHub repository and Streamlit app (see below) for the compulsory work.\n",
    "\n",
    "Document headings should be clear and usable for navigation during development.\n",
    "\n",
    "All code blocks must include enough comments to be understandable and reproducible if someone inherits your project.\n",
    "\n",
    "All code blocks must be run before an export to PDF so the messages and plots are shown. In addition, add the .ipynb file to the GitHub repository where you have your Streamlit project.\n",
    "\n",
    "\n",
    "#### Tasks for assignment 2\n",
    "Use the Elhub API to retrieve hourly production data for all price areas using PRODUCTION_PER_GROUP_MBA_HOUR for all days and hours of the year 2021.\n",
    "\n",
    "Extract only the list in productionPerGroupMbaHour, convert to a DataFrame, and insert the data into Cassandra using Spark.\n",
    "\n",
    "Use Spark to extract the columns priceArea, productionGroup, startTime, and quantityKwh from Cassandra.\n",
    "\n",
    "Create the following plots:\n",
    "- A pie chart for the total production of the year from a chosen price area, where each piece of the pie is one of the production groups.\n",
    "- A line plot for the first month of the year for a chosen price area. Make separate lines for each production group.\n",
    "\n",
    "Insert the Spark-extracted data into your MongoDB.\n",
    "\n",
    "Remember to fill in the log and AI mentioned in the General section above.\n",
    "\n",
    "### Streamlit app\n",
    "Establish a connection with your MongoDB database. When running this at streamlit.io, remember to copy your secrets to the webpage instead of exposing them on GitHub.\n",
    "\n",
    "On page four, split the view into two columns using st.columns.\n",
    "\n",
    "- On the left side, use radio buttons (st.radio) to select a price area and display a pie chart like in the Jupyter Notebook\n",
    "\n",
    "- On the right side, use pills (st.pills) to select which production groups to include and a selection element of your choice to select a month. Combine the price area, production group(s) and month, and display a line plot like in the Jupyter Notebook (but for any month).\n",
    "\n",
    "- Below the columns, insert an expander (st.expander) where you briefly document the source of the data shown on the page."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IND320_SMAA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
