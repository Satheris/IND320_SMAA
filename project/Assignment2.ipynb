{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9606477",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d72be5",
   "metadata": {},
   "source": [
    "## AI usage \n",
    "\n",
    "I generally prefer coding by myself over using AI for help. For this assignment I did, however, find it necessary to use AI to increase my efficiency. \n",
    "\n",
    "Used DeepSeek for help on how to connect Cassandra and Spark. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e9c323",
   "metadata": {},
   "source": [
    "## Log \n",
    "\n",
    "For this assignment I will try to work more systematic than the last. I will focus on finishing all the elements for the notebook first, and work on the streamlit app afterwards. \n",
    "\n",
    "I started by connecting Cassandra and Spark, but ran into problems regarding the sparksession. After troubleshooting for a while and getting some help, we ended up using DeepSeek for help on how to maybe fix it. DeepSeek suggested adding the last three lines of code to force sessionbuilder to use the localhost. \n",
    "\n",
    "I have struggled a bit with connecting to Spark and MongoDB, especially lagging a couple weeks behind on lectures. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c299533c",
   "metadata": {},
   "source": [
    "## Links \n",
    "\n",
    "- Github: https://github.com/Satheris/IND320_SMAA\n",
    "- Streamlit app: https://ind320smaa-2eg32uba6uhmrknkwtxzar.streamlit.app/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f530005e",
   "metadata": {},
   "source": [
    "## Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2d9397",
   "metadata": {},
   "source": [
    "### Imports and system variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d5b8e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import streamlit as st\n",
    "import pymongo\n",
    "from cassandra.cluster import Cluster\n",
    "from pyspark.sql import SparkSession\n",
    "from pyjstat import pyjstat\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46f781e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables for PySpark (system and version dependent!) \n",
    "# if not already set persistently (e.g., in .bashrc or .bash_profile or Windows environment variables)\n",
    "import os\n",
    "# Set the Java home path to the one you are using ((un)comment and edit as needed):\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Java\\jre1.8.0_471\"\n",
    "\n",
    "# If you are using environments in Python, you can set the environment variables like the alternative below.\n",
    "# The default Python environment is used if the variables are set to \"python\" (edit if needed):\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python\" # or similar to \"/Users/kristian/miniforge3/envs/tf_M1/bin/python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python\" # or similar to \"/Users/kristian/miniforge3/envs/tf_M1/bin/python\"\n",
    "\n",
    "# On Windows you need to specify where the Hadoop drivers are located (uncomment and edit if needed):\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\Users\\saraa\\Documents\\winutils\\hadoop-3.3.1\"\n",
    "\n",
    "# Set the Hadoop version to the one you are using, e.g., none:\n",
    "os.environ[\"PYSPARK_HADOOP_VERSION\"] = \"without\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa36443",
   "metadata": {},
   "source": [
    "### Cassandra and Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbdc9774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to Cassandra\n",
    "cluster = Cluster(['localhost'], port=9042)\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec95e2a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x23c590ad8d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up new keyspace\n",
    "#                                              name of keyspace                        replication strategy           replication factor\n",
    "session.execute(\"CREATE KEYSPACE IF NOT EXISTS ind320_keyspace WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\")\n",
    "\n",
    "# Create a new table\n",
    "session.set_keyspace('ind320_keyspace')\n",
    "session.execute(\"DROP TABLE IF EXISTS ind320_keyspace.elhub_api;\") # Starting from scratch every time\n",
    "session.execute(\"CREATE TABLE IF NOT EXISTS elhub_api (ind timeuuid PRIMARY KEY, endTime text, lastUpdatedTime text,\" \\\n",
    "                \"priceArea text, productionGroup text, quantityKwh float, startTime text);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "594220f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('SparkCassandraApp').\\\n",
    "    config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.5.1').\\\n",
    "    config('spark.cassandra.connection.host', 'localhost').\\\n",
    "    config('spark.sql.extensions', 'com.datastax.spark.connector.CassandraSparkExtensions').\\\n",
    "    config('spark.sql.catalog.mycatalog', 'com.datastax.spark.connector.datasource.CassandraCatalog').\\\n",
    "    config('spark.cassandra.connection.port', '9042').\\\n",
    "    config('spark.driver.host', 'localhost').\\\n",
    "    config('spark.driver.bindAddress', '127.0.0.1').\\\n",
    "    config('spark.sql.adaptive.enabled', 'true').\\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d608437",
   "metadata": {},
   "source": [
    "#### Testing that the connection works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96b67059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+\n",
      "|ind| company|  model|\n",
      "+---+--------+-------+\n",
      "|  1|   Tesla|Model S|\n",
      "|  2|   Tesla|Model 3|\n",
      "|460|    Ford|Transit|\n",
      "|459|    Ford| Escort|\n",
      "|  3|Polestar|      3|\n",
      "+---+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# .load() is used to load data from Cassandra table as a Spark DataFrame\n",
    "spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"my_first_table\", keyspace=\"my_first_keyspace\").load().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f7c48da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+\n",
      "| planet| distance| diameter|\n",
      "+-------+---------+---------+\n",
      "|Mercury| 0.387 AU|  4878 km|\n",
      "|  Venus| 0.723 AU| 12104 km|\n",
      "|  Earth| 1.000 AU| 12756 km|\n",
      "|   Mars| 1.524 AU|  6787 km|\n",
      "|Jupiter| 5.203 AU|142796 km|\n",
      "| Saturn| 9.546 AU|120660 km|\n",
      "| Uranus|19.218 AU| 51118 km|\n",
      "|Neptune|30.069 AU| 48600 km|\n",
      "+-------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read CSV file into Spark DataFrame\n",
    "planets = spark.read.csv(\"../data/planets.csv\", header=True, inferSchema=True)\n",
    "planets.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a89b09",
   "metadata": {},
   "source": [
    "### MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f1f4a17",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConfigurationError",
     "evalue": "The DNS operation timed out after 20.01268768310547 seconds",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTimeout\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saraa\\anaconda3\\envs\\IND320_SMAA\\Lib\\site-packages\\pymongo\\srv_resolver.py:72\u001b[39m, in \u001b[36m_SrvResolver._resolve_uri\u001b[39m\u001b[34m(self, encapsulate_errors)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     results = \u001b[43mresolver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m_mongodb._tcp.\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__fqdn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mSRV\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mlifetime\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__connect_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saraa\\anaconda3\\envs\\IND320_SMAA\\Lib\\site-packages\\dns\\resolver.py:1100\u001b[39m, in \u001b[36mquery\u001b[39m\u001b[34m(qname, rdtype, rdclass, tcp, source, raise_on_no_answer, source_port, lifetime)\u001b[39m\n\u001b[32m   1091\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Query nameservers to find the answer to the question.\u001b[39;00m\n\u001b[32m   1092\u001b[39m \n\u001b[32m   1093\u001b[39m \u001b[33;03mThis is a convenience function that uses the default resolver\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1097\u001b[39m \u001b[33;03mparameters.\u001b[39;00m\n\u001b[32m   1098\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1100\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_default_resolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrdclass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtcp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1101\u001b[39m \u001b[43m                                    \u001b[49m\u001b[43mraise_on_no_answer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[43m                                    \u001b[49m\u001b[43mlifetime\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saraa\\anaconda3\\envs\\IND320_SMAA\\Lib\\site-packages\\dns\\resolver.py:992\u001b[39m, in \u001b[36mResolver.query\u001b[39m\u001b[34m(self, qname, rdtype, rdclass, tcp, source, raise_on_no_answer, source_port, lifetime)\u001b[39m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(nameservers) > \u001b[32m0\u001b[39m:\n\u001b[32m    988\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    989\u001b[39m     \u001b[38;5;66;03m# But we still have servers to try.  Sleep a bit\u001b[39;00m\n\u001b[32m    990\u001b[39m     \u001b[38;5;66;03m# so we don't pound them!\u001b[39;00m\n\u001b[32m    991\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m992\u001b[39m     timeout = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlifetime\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    993\u001b[39m     sleep_time = \u001b[38;5;28mmin\u001b[39m(timeout, backoff)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saraa\\anaconda3\\envs\\IND320_SMAA\\Lib\\site-packages\\dns\\resolver.py:799\u001b[39m, in \u001b[36mResolver._compute_timeout\u001b[39m\u001b[34m(self, start, lifetime)\u001b[39m\n\u001b[32m    798\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m duration >= lifetime:\n\u001b[32m--> \u001b[39m\u001b[32m799\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Timeout(timeout=duration)\n\u001b[32m    800\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lifetime - duration, \u001b[38;5;28mself\u001b[39m.timeout)\n",
      "\u001b[31mTimeout\u001b[39m: The DNS operation timed out after 20.01268768310547 seconds",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConfigurationError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minit_connection\u001b[39m():\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pymongo.MongoClient(st.secrets[\u001b[33m\"\u001b[39m\u001b[33mmongo\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33muri\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m client = \u001b[43minit_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Send a ping to confirm a successful connection\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36minit_connection\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minit_connection\u001b[39m():\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpymongo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMongoClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mst\u001b[49m\u001b[43m.\u001b[49m\u001b[43msecrets\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmongo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muri\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saraa\\anaconda3\\envs\\IND320_SMAA\\Lib\\site-packages\\pymongo\\mongo_client.py:639\u001b[39m, in \u001b[36mMongoClient.__init__\u001b[39m\u001b[34m(self, host, port, document_class, tz_aware, connect, type_registry, **kwargs)\u001b[39m\n\u001b[32m    636\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    637\u001b[39m     timeout = common.validate_timeout_or_none_or_zero(\n\u001b[32m    638\u001b[39m         keyword_opts.cased_key(\u001b[33m\"\u001b[39m\u001b[33mconnecttimeoutms\u001b[39m\u001b[33m\"\u001b[39m), timeout)\n\u001b[32m--> \u001b[39m\u001b[32m639\u001b[39m res = \u001b[43muri_parser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse_uri\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m    \u001b[49m\u001b[43mentity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconnect_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    642\u001b[39m seeds.update(res[\u001b[33m\"\u001b[39m\u001b[33mnodelist\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    643\u001b[39m username = res[\u001b[33m\"\u001b[39m\u001b[33musername\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m username\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saraa\\anaconda3\\envs\\IND320_SMAA\\Lib\\site-packages\\pymongo\\uri_parser.py:500\u001b[39m, in \u001b[36mparse_uri\u001b[39m\u001b[34m(uri, default_port, validate, warn, normalize, connect_timeout)\u001b[39m\n\u001b[32m    498\u001b[39m connect_timeout = connect_timeout \u001b[38;5;129;01mor\u001b[39;00m options.get(\u001b[33m\"\u001b[39m\u001b[33mconnectTimeoutMS\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    499\u001b[39m dns_resolver = _SrvResolver(fqdn, connect_timeout=connect_timeout)\n\u001b[32m--> \u001b[39m\u001b[32m500\u001b[39m nodes = \u001b[43mdns_resolver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_hosts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m dns_options = dns_resolver.get_options()\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dns_options:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saraa\\anaconda3\\envs\\IND320_SMAA\\Lib\\site-packages\\pymongo\\srv_resolver.py:102\u001b[39m, in \u001b[36m_SrvResolver.get_hosts\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_hosts\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     _, nodes = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_srv_response_and_hosts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m nodes\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saraa\\anaconda3\\envs\\IND320_SMAA\\Lib\\site-packages\\pymongo\\srv_resolver.py:83\u001b[39m, in \u001b[36m_SrvResolver._get_srv_response_and_hosts\u001b[39m\u001b[34m(self, encapsulate_errors)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_srv_response_and_hosts\u001b[39m(\u001b[38;5;28mself\u001b[39m, encapsulate_errors):\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_resolve_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencapsulate_errors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m     \u001b[38;5;66;03m# Construct address tuples\u001b[39;00m\n\u001b[32m     86\u001b[39m     nodes = [\n\u001b[32m     87\u001b[39m         (maybe_decode(res.target.to_text(omit_final_dot=\u001b[38;5;28;01mTrue\u001b[39;00m)), res.port)\n\u001b[32m     88\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saraa\\anaconda3\\envs\\IND320_SMAA\\Lib\\site-packages\\pymongo\\srv_resolver.py:79\u001b[39m, in \u001b[36m_SrvResolver._resolve_uri\u001b[39m\u001b[34m(self, encapsulate_errors)\u001b[39m\n\u001b[32m     77\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m     78\u001b[39m     \u001b[38;5;66;03m# Else, raise all errors as ConfigurationError.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ConfigurationError(\u001b[38;5;28mstr\u001b[39m(exc))\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[31mConfigurationError\u001b[39m: The DNS operation timed out after 20.01268768310547 seconds"
     ]
    }
   ],
   "source": [
    "# Read user and password\n",
    "with open('.streamlit\\secrets.toml', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "def init_connection():\n",
    "    return pymongo.MongoClient(st.secrets[\"mongo\"][\"uri\"])\n",
    "\n",
    "client = init_connection()\n",
    "\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d4285c",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://api.elhub.no/energy-data/v0/price-areas?dataset=PRODUCTION_PER_GROUP_MBA_HOUR&startDate=2021-01-01T00:00:00%2B02:00&endDate=2021-02-01T00:00:00%2B02:00'\n",
    "\n",
    "payload = { \n",
    "    \"query\": [], \n",
    "    \"response\": { \"format\": \"json-stat2\" } }\n",
    "\n",
    "response = requests.get(URL, json=payload)\n",
    "data = response.json()\n",
    "\n",
    "# Writing the data into a file\n",
    "with open(r'data\\api_response.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(response.json(), f, indent=2, ensure_ascii=False)\n",
    "print(\"Response saved to 'api_response.json'\")\n",
    "\n",
    "\n",
    "# Prints for status\n",
    "print(\"\\nStatus Code:\", response.status_code)\n",
    "print(\"Headers:\", response.headers.get('content-type'))\n",
    "\n",
    "\n",
    "# Extract all production records\n",
    "all_records = []\n",
    "for area in data['data']:\n",
    "    records = area['attributes']['productionPerGroupMbaHour']\n",
    "    for record in records:\n",
    "        record['priceArea'] = area['attributes']['name']  # Add area name\n",
    "        all_records.append(record)\n",
    "\n",
    "df = pd.DataFrame(all_records)\n",
    "print(f\"\\nCreated DataFrame with {len(df)} rows\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa56e71",
   "metadata": {},
   "source": [
    "Now that I successfullt imported 1 month, I need to import for all twelve months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9766279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthStart = ['2021-01-01T00:00:00', '2021-02-01T00:00:00', '2021-03-01T00:00:00',\n",
    "              '2021-04-01T00:00:00', '2021-05-01T00:00:00', '2021-06-01T00:00:00',\n",
    "              '2021-07-01T00:00:00', '2021-08-01T00:00:00', '2021-09-01T00:00:00',\n",
    "              '2021-10-01T00:00:00', '2021-11-01T00:00:00', '2021-12-01T00:00:00',\n",
    "              '2022-01-01T00:00:00']\n",
    "\n",
    "\n",
    "all_records = []\n",
    "\n",
    "for i, month in enumerate(monthStart[:12]):\n",
    "    URL = f'https://api.elhub.no/energy-data/v0/price-areas?dataset=PRODUCTION_PER_GROUP_MBA_HOUR&startDate={month}%2B02:00&endDate={monthStart[i+1]}%2B02:00'\n",
    "\n",
    "    payload = { \n",
    "    \"query\": [], \n",
    "    \"response\": { \"format\": \"json-stat2\" } }\n",
    "\n",
    "    response = requests.get(URL, json=payload)\n",
    "    data = response.json()\n",
    "\n",
    "    for area in data['data']:\n",
    "        records = area['attributes']['productionPerGroupMbaHour']\n",
    "        for record in records:\n",
    "            record['priceArea'] = area['attributes']['name']  # Add area name\n",
    "            all_records.append(record)\n",
    "\n",
    "df = pd.DataFrame(all_records)\n",
    "print(f\"\\nCreated DataFrame with {len(df)} rows\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86801553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Pandas DataFrame to Spark DataFrame and save it to Cassandra (append mode)\n",
    "spark.createDataFrame(df).write.format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".options(table=\"elhub_api\", keyspace=\"ind320_keyspace\").mode(\"append\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb45621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "151ae79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session terminated successfully\n"
     ]
    }
   ],
   "source": [
    "# Stop Spark session\n",
    "try:\n",
    "    spark.stop()\n",
    "    print('Spark session terminated successfully')\n",
    "except ConnectionRefusedError:\n",
    "    print(\"Spark session already stopped.\")\n",
    "except NameError:\n",
    "    print('Spark session is not defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b66397",
   "metadata": {},
   "source": [
    "# General\n",
    "\n",
    "\n",
    "A Streamlit app running from https://[yourproject].streamlit.app/.\n",
    "This is an online version of the project, accessing data that has been exported to CSV format and accessing your MongoDB database for additional data.\n",
    "The code, hosted at GitHub, must include relevant comments from the Jupyter Notebook and further comments regarding Streamlit usage.\n",
    "\n",
    "\n",
    "## Tasks\n",
    "\n",
    "### Local database: Cassandra\n",
    "If not already done, set up Cassandra and Spark as described in the book.\n",
    "Test that your Spark-Cassandra connection works.\n",
    "The Cassandra database will be accessed from the Jupyter Notebook and used to store data from the API mentioned later. \n",
    "\n",
    "### Remote database: MongoDB\n",
    "If not already done, prepare a MongoDB account at mongodb.com.\n",
    "Test that you can manipulate data from Python.\n",
    "The MongoDB database will store data that has been trimmed/curated/prepared through the Jupyter Notebook and Spark filtering.\n",
    "These data will be accessed directly from the Streamlit app.\n",
    "\n",
    "### API\n",
    "Familiarise yourself with the API connection at https://api.elhub.noLenker til en ekstern side.\n",
    "\n",
    "Observe how time is encoded and how transitions between summer and winter time are handled.\n",
    "Be aware of the time period limitations for each API request and how this differs between datasets.\n",
    "\n",
    "\n",
    "### Jupyter Notebook\n",
    "\n",
    "#### Standard requirements\n",
    "\n",
    "Must include a brief description of AI usage.\n",
    "\n",
    "Must include a 300-500-word log describing the compulsory work (including both Jupyter Notebook and Streamlit experience).\n",
    "\n",
    "Must include links to your public GitHub repository and Streamlit app (see below) for the compulsory work.\n",
    "\n",
    "Document headings should be clear and usable for navigation during development.\n",
    "\n",
    "All code blocks must include enough comments to be understandable and reproducible if someone inherits your project.\n",
    "\n",
    "All code blocks must be run before an export to PDF so the messages and plots are shown. In addition, add the .ipynb file to the GitHub repository where you have your Streamlit project.\n",
    "\n",
    "\n",
    "#### Tasks for assignment 2\n",
    "Use the Elhub API to retrieve hourly production data for all price areas using PRODUCTION_PER_GROUP_MBA_HOUR for all days and hours of the year 2021.\n",
    "\n",
    "Extract only the list in productionPerGroupMbaHour, convert to a DataFrame, and insert the data into Cassandra using Spark.\n",
    "\n",
    "Use Spark to extract the columns priceArea, productionGroup, startTime, and quantityKwh from Cassandra.\n",
    "\n",
    "Create the following plots:\n",
    "- A pie chart for the total production of the year from a chosen price area, where each piece of the pie is one of the production groups.\n",
    "- A line plot for the first month of the year for a chosen price area. Make separate lines for each production group.\n",
    "\n",
    "Insert the Spark-extracted data into your MongoDB.\n",
    "\n",
    "Remember to fill in the log and AI mentioned in the General section above.\n",
    "\n",
    "### Streamlit app\n",
    "Establish a connection with your MongoDB database. When running this at streamlit.io, remember to copy your secrets to the webpage instead of exposing them on GitHub.\n",
    "\n",
    "On page four, split the view into two columns using st.columns.\n",
    "\n",
    "- On the left side, use radio buttons (st.radio) to select a price area and display a pie chart like in the Jupyter Notebook\n",
    "\n",
    "- On the right side, use pills (st.pills) to select which production groups to include and a selection element of your choice to select a month. Combine the price area, production group(s) and month, and display a line plot like in the Jupyter Notebook (but for any month).\n",
    "\n",
    "- Below the columns, insert an expander (st.expander) where you briefly document the source of the data shown on the page."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IND320_SMAA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
